# ğŸ›¡ï¸ PromptGuard

PromptGuard is a **full-stack AI safety system** designed to analyze user prompts and prevent unsafe or unethical requests **before** they reach generative AI models.

It combines **rule-based filtering**, **machine learning risk scoring**, and **explainable decisions** in a clean, production-style architecture.

---

## ğŸš€ Why PromptGuard?

Recent incidents involving generative AI misuse highlight the need for **pre-generation safety layers**.

PromptGuard focuses on:
- Preventing harmful prompts (e.g., explicit, abusive, exploitative)
- Providing **transparent reasons** for decisions
- Suggesting **safe rewrites** instead of hard blocking when possible

---

## ğŸ§  How It Works

1. **User enters a prompt**
2. **Rule-based checks** detect explicit violations
3. **ML model** computes a risk score
4. **Decision engine** determines:
   - âœ… SAFE
   - âœï¸ REWRITE
   - âŒ BLOCK
5. User receives:
   - Decision
   - Risk score
   - Explanation
   - Suggested rewrite (if applicable)

---

## ğŸ—ï¸ Tech Stack

### Frontend
- React (Vite)
- Tailwind CSS v4
- Dark mode with persistent theme toggle

### Backend
- FastAPI
- Hugging Face Transformers
- Rule-based + ML hybrid logic

---

## ğŸ“‚ Project Structure
PromptGuard/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app.py # FastAPI entry point
â”‚ â”œâ”€â”€ model.py # ML risk scoring
â”‚ â”œâ”€â”€ rules.py # Rule-based checks
â”‚ â”œâ”€â”€ decision.py # Final decision logic
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ promptguard-ui/
â”‚ â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ index.html
â”‚ â”œâ”€â”€ tailwind.config.js
â”‚ â””â”€â”€ package.json
â””â”€â”€ README.md
